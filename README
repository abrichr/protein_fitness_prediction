# Goals

Please use the provided subset of this sequence->fitness dataset
(representing 0.3% of the possible sequence diversity) to:

- construct 10 new sequences outside of the provided dataset with the highest
predicted fitness for a subsequent round of synthesis.

- Suggest 10 compounds outside of the dataset provided that if you had
experimental data on would most improve your model (and thus answer
for #1). 

# Implementation 

This implementation resembles that of
[innov’SAR](https://www.genome.jp/aaindex/).

Steps:
1) Encoding

    a) [AAIndex](https://www.genome.jp/aaindex/)1 features via
    [PyBioMed](https://github.com/gadsbyfly/PyBioMed):

    > This database holds more than 500 numerical indices representing various
    > physicochemical and biochemical properties for the 20 standard amino
    > acids and correlations between these indices are also listed.

    TODO: use correlations

    b) Fast Fourier Transform (FFT) via `numpy.fft`

2) Modelling

    > the modelling phase, will use the experimental values of the target
    > activity, in conjunction with these protein spectra, in order to identify
    > a predictive model.
    > The model is constructed by the application of standard regression
    > approaches based on a learning step and a validation step. innov’SAR
    > used a partial least square regression, PLS, as algorithm of regression
    > to do the model for the predictions of the enantioselectivity of epoxide
    > hydrolase. 

    This implementation differs in the following ways:

    - Predict on given fitness values (instead of enantioselectivity of epoxide
    hydrolase)
    - Evaluates 17 models from [sklearn](https://scikit-learn.org) in addition
    to PLS.
    - Runs hyperparameter search

3) Validation

    > The root mean squared error (RMSE) and the coefficient of determination
    > (R2) are the performance parameters to assess a regression model, during
    > the validation step.

    The cross validation procedure described in the paper is implemented here
    with sklearn.

# Discussion

The best R2 value achieved was ~-2, i.e. less effective
than the null hypothesis of predicting the mean. This may be due to
insufficiently exhaustive hyperparameter search. Alternatively it may be
because this model was only trained on 0.3% of the data, whereas the reference
paper trained on all of it.

# Future Work

Perhaps it is possible to reduce the search space of the given training
set by creating a mutation graph of the given variants, and only keeping
the largest connected component.
